{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"assets/pipelines_img.jpg\" width=400 height=100 />\n",
    "\n",
    "<center> <a href=\"https://unsplash.com/photos/Bu4lHKIHr-E\">Source</a> </center>\n",
    "\n",
    "# Pipeline for Data Validation and Model Building and Serving\n",
    "\n",
    "Author: CHNG Soon Siang ([LinkedIn](https://www.linkedin.com/in/soon-siang-chng/))<br>\n",
    "Dataset: [Heart Disease Dataset](https://www.kaggle.com/ronitf/heart-disease-uci)\n",
    "\n",
    "Date last updated: 16 Oct 2021\n",
    "\n",
    "_**Summary**_    \n",
    "\n",
    "Machine Learning Operation (MLOps) is the marriage of DevOps with Machine Learning, to bring a machine learning model into production. In this notebook, with codes adapted from various sources, I explored how one can utilize a popular MLOps framework - Tensorflow Extended (TFX) - to do end-to-end machine learning on the Heart Disease dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Background\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-16T09:35:31.049025Z",
     "start_time": "2021-10-16T09:35:31.004145Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pprint\n",
    "import numpy as np\n",
    "import urllib\n",
    "import tempfile\n",
    "import cloudpickle\n",
    "\n",
    "import absl\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import tensorflow_model_analysis as tfma\n",
    "tf.get_logger().propagate = False\n",
    "pp = pprint.PrettyPrinter()\n",
    "\n",
    "import tfx\n",
    "\n",
    "import tensorflow_data_validation as tfdv\n",
    "import tensorflow_transform as tft\n",
    "\n",
    "# TFX Components\n",
    "from tfx.components import CsvExampleGen\n",
    "from typing import Dict, List, Text\n",
    "from tfx.components import Evaluator, ExampleValidator, Pusher, SchemaGen, StatisticsGen, Trainer, Transform\n",
    "from tfx.components.base import executor_spec\n",
    "from tfx.components.trainer.executor import GenericExecutor\n",
    "from tfx.dsl.experimental import latest_blessed_model_resolver\n",
    "from google.protobuf.json_format import MessageToDict\n",
    "\n",
    "# TFX Orchestrator and Proto\n",
    "from tfx.orchestration import metadata, pipeline\n",
    "from tfx.orchestration.experimental.interactive.interactive_context import InteractiveContext\n",
    "from tfx.proto import example_gen_pb2, pusher_pb2, trainer_pb2\n",
    "from tfx.types import Channel\n",
    "from tfx.types.standard_artifacts import Model, ModelBlessing\n",
    "\n",
    "# Utilities\n",
    "from tensorflow.python.lib.io import file_io\n",
    "from tensorflow_metadata.proto.v0 import schema_pb2\n",
    "from google.protobuf.json_format import MessageToDict\n",
    "from tfx.types import standard_artifacts\n",
    "import apache_beam as beam\n",
    "\n",
    "# Display versions of TF and TFX related packages\n",
    "print('TensorFlow version: {}'.format(tf.__version__))\n",
    "print('TFX version: {}'.format(tfx.__version__))\n",
    "print('TensorFlow Data Validation version: {}'.format(tfdv.__version__))\n",
    "print('TensorFlow Transform version: {}'.format(tft.__version__))\n",
    "print(f'Apache Beam version: {beam.__version__}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-16T09:35:34.075792Z",
     "start_time": "2021-10-16T09:35:33.734562Z"
    }
   },
   "outputs": [],
   "source": [
    "!mkdir {os.path.join('.', 'HD_BNN_tfx')}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-16T09:35:35.911577Z",
     "start_time": "2021-10-16T09:35:35.879688Z"
    }
   },
   "outputs": [],
   "source": [
    "## Define all constant\n",
    "\n",
    "_tfx_root = os.path.abspath(os.path.join('.', 'HD_BNN_tfx'))     # Create location ~/tfx\n",
    "_pipeline_root = os.path.join(_tfx_root, 'pipelines')   # Join ~/tfx/pipelines/\n",
    "_metadata_db_root = os.path.join(_tfx_root, 'metadata.db')  # Join ~/tfx/metadata.db\n",
    "_log_root = os.path.join(_tfx_root, 'logs')\n",
    "_model_root = os.path.join(_tfx_root, 'model')\n",
    "_data_root = os.path.join(_tfx_root, 'data')\n",
    "_serving_model_dir = os.path.join(_tfx_root, 'serving_model')\n",
    "_data_filepath = os.path.join(_data_root, \"heart.csv\")\n",
    "\n",
    "# Modules for Transform component\n",
    "_constant_transform_module_file = 'constants_transform.py'\n",
    "_features_transform_module_file = 'features_transform.py'\n",
    "\n",
    "# Modules for Trainer component\n",
    "_input_fn_module_file = 'inputfn_trainer.py'\n",
    "_constants_train_module_file = 'constants_trainer.py'\n",
    "_model_trainer_module_file = 'model_trainer.py'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-16T09:35:37.652233Z",
     "start_time": "2021-10-16T09:35:37.564985Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "context = InteractiveContext(pipeline_root=_tfx_root)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ExampleGen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-16T05:55:58.116758Z",
     "start_time": "2021-10-16T05:55:54.928527Z"
    }
   },
   "outputs": [],
   "source": [
    "# Instantiate ExampleGen with the input CSV dataset\n",
    "example_gen = CsvExampleGen(\n",
    "    input_base = _data_root\n",
    ")\n",
    "\n",
    "# Run the component using the InteractiveContext instance\n",
    "context.run(example_gen, enable_cache = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-16T05:55:59.344062Z",
     "start_time": "2021-10-16T05:55:59.330089Z"
    }
   },
   "outputs": [],
   "source": [
    "len(example_gen.outputs['examples'].get()), type(example_gen.outputs['examples'].get())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-16T05:56:00.052847Z",
     "start_time": "2021-10-16T05:56:00.043869Z"
    }
   },
   "outputs": [],
   "source": [
    "pp.pprint(example_gen.outputs['examples'].get()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-16T05:56:03.553240Z",
     "start_time": "2021-10-16T05:56:03.538271Z"
    }
   },
   "outputs": [],
   "source": [
    "# get the artifact object\n",
    "artifact = example_gen.outputs['examples'].get()[0]\n",
    "\n",
    "# print split names and uri\n",
    "print(f'split names: {artifact.split_names}')\n",
    "print(f'artifact uri: {artifact.uri}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-16T05:56:05.099957Z",
     "start_time": "2021-10-16T05:56:04.311200Z"
    }
   },
   "outputs": [],
   "source": [
    "# Get the URI of the output artifact representing the training examples, which is a directory\n",
    "train_uri = os.path.join(artifact.uri, 'Split-train') # train -> Split-train\n",
    "\n",
    "# Get the list of files in this directory (all compressed TFRecord files)\n",
    "tfrecord_filenames = [os.path.join(train_uri, name)\n",
    "                      for name in os.listdir(train_uri)]\n",
    "\n",
    "# Create a `TFRecordDataset` to read these files\n",
    "dataset = tf.data.TFRecordDataset(tfrecord_filenames, compression_type=\"GZIP\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-16T05:56:05.114922Z",
     "start_time": "2021-10-16T05:56:05.100956Z"
    }
   },
   "outputs": [],
   "source": [
    "# Utility function to convert TFRecords into dictionary for easy visualisation.\n",
    "\n",
    "def get_records(dataset, num_records):\n",
    "    '''Extracts records from the given dataset.\n",
    "    Args:\n",
    "        dataset (TFRecordDataset): dataset saved by ExampleGen\n",
    "        num_records (int): number of records to preview\n",
    "    '''\n",
    "    \n",
    "    # initialize an empty list\n",
    "    records = []\n",
    "\n",
    "    # Use the `take()` method to specify how many records to get\n",
    "    for tfrecord in dataset.take(num_records):\n",
    "        \n",
    "        # Get the numpy property of the tensor\n",
    "        serialized_example = tfrecord.numpy()\n",
    "        \n",
    "        # Initialize a `tf.train.Example()` to read the serialized data\n",
    "        example = tf.train.Example()\n",
    "        \n",
    "        # Read the example data (output is a protocol buffer message)\n",
    "        example.ParseFromString(serialized_example)\n",
    "        \n",
    "        # convert the protocol bufffer message to a Python dictionary\n",
    "        example_dict = (MessageToDict(example))\n",
    "        \n",
    "        # append to the records list\n",
    "        records.append(example_dict)\n",
    "        \n",
    "    return records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-16T05:56:06.336313Z",
     "start_time": "2021-10-16T05:56:06.162976Z"
    }
   },
   "outputs": [],
   "source": [
    "pp.pprint(get_records(dataset, 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## StatisticsGen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-16T05:56:14.181363Z",
     "start_time": "2021-10-16T05:56:09.340468Z"
    }
   },
   "outputs": [],
   "source": [
    "# Instantiate StatisticsGen with the ExampleGen ingested dataset\n",
    "statistics_gen = StatisticsGen( # Instantiate a StatisticsGen class\n",
    "    examples=example_gen.outputs['examples'] # Insert the outputs of example_gen component into StatisticsGen\n",
    ")\n",
    "    \n",
    "# Run the component\n",
    "context.run(statistics_gen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-16T05:56:14.244383Z",
     "start_time": "2021-10-16T05:56:14.182361Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Plot the statistics generated\n",
    "context.show(statistics_gen.outputs['statistics'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SchemaGen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-16T05:56:24.579225Z",
     "start_time": "2021-10-16T05:56:23.935974Z"
    }
   },
   "outputs": [],
   "source": [
    "# Instantiate SchemaGen with the output statistics from the StatisticsGen\n",
    "schema_gen = SchemaGen(\n",
    "    statistics = statistics_gen.outputs['statistics'],\n",
    "    # How to know the key is 'statistics'? Can view from the context.run output. >>>\n",
    "    #.component.outputs\t['statistics']\tChannel of type 'ExampleStatistics' (1 artifact) at 0x7f1628fb09a0\n",
    ")\n",
    "    \n",
    "# Run the component\n",
    "context.run(schema_gen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-16T05:56:25.360352Z",
     "start_time": "2021-10-16T05:56:25.342433Z"
    }
   },
   "outputs": [],
   "source": [
    "schema_gen.outputs['schema']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-16T05:56:25.987885Z",
     "start_time": "2021-10-16T05:56:25.949986Z"
    }
   },
   "outputs": [],
   "source": [
    "# Visualize the output\n",
    "context.show(schema_gen.outputs['schema'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What does 'presence' and 'valency' mean?\n",
    "\n",
    "Presence indicates whether the feature must be present in 100% of examples (required) or not (optional).\n",
    "Valency indicates the number of values required per training example. In the case of categorical features, single indicates that each training example must have exactly one category for the feature. [Source](https://dzlab.github.io/ml/2020/11/03/tfx-data-validation/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-16T05:56:27.285409Z",
     "start_time": "2021-10-16T05:56:27.277430Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "schema_gen.outputs['schema']._artifacts[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-16T05:56:27.892615Z",
     "start_time": "2021-10-16T05:56:27.882576Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "schema_gen.outputs['schema']._artifacts[0].uri"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-16T05:56:28.442207Z",
     "start_time": "2021-10-16T05:56:28.435220Z"
    }
   },
   "outputs": [],
   "source": [
    "schema_uri = schema_gen.outputs['schema']._artifacts[0].uri"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-16T05:56:28.945765Z",
     "start_time": "2021-10-16T05:56:28.935819Z"
    }
   },
   "outputs": [],
   "source": [
    "# Get the schema pbtxt file from the SchemaGen output\n",
    "schema = tfdv.load_schema_text(os.path.join(schema_uri, 'schema.pbtxt'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The domain for each feature within the schema is empty, will be good to update them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-16T05:56:29.874188Z",
     "start_time": "2021-10-16T05:56:29.860227Z"
    }
   },
   "outputs": [],
   "source": [
    "feature_names = ['age',\n",
    "'sex',\n",
    "'cp',\n",
    "'trtbps',\n",
    "'chol',\n",
    "'fbs',\n",
    "'restecg',\n",
    "'thalachh',\n",
    "'exng',\n",
    "'oldpeak',\n",
    "'slp',\n",
    "'caa',\n",
    "'thall',\n",
    "'output']\n",
    "\n",
    "categorical_features = ['sex', 'cp', 'fbs', 'restecg', 'exng', 'slp', 'caa', 'thall']\n",
    "\n",
    "numerical_features = list(set(feature_names) - set(categorical_features))\n",
    "\n",
    "numerical_features\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The features of the dataset and its respective description\n",
    "We can assume that information on the features, such as whether the feature is categorical or numerical, or the domain of the feature is provided in the dataset metadata. In the future, we will explore how to attain this information by exploring the dataset using TFX. This is given under the `asset` folder as `heart_disease_features_descript.csv`.\n",
    "\n",
    "| Feature  | Description                                                    | Type  | NumOrCat | Domain      | Domain_min | Domain_max |\n",
    "|----------|----------------------------------------------------------------|-------|----------|-------------|------------|------------|\n",
    "| age      | age in years                                                   | int   | Num      | na          | na         | na         |\n",
    "| sex      | (1 = male; 0 = female)                                         | int   | Cat      | [1 0]       | 0          | 1          |\n",
    "| cp       | chest pain type                                                | int   | Cat      | [0 1 2 3]   | 0          | 3          |\n",
    "| trtbps   | resting blood pressure (in mm Hg on admission to the hospital) | int   | Num      | na          | na         | na         |\n",
    "| chol     | serum cholestoral in mg/dl                                     | int   | Num      | na          | na         | na         |\n",
    "| fbs      | (fasting blood sugar &gt; 120 mg/dl) (1 = true; 0 = false)     | int   | Cat      | [1 0]       | 0          | 1          |\n",
    "| restecg  | resting electrocardiographic results                           | int   | Cat      | [0 1 2]     | 0          | 2          |\n",
    "| thalachh | maximum heart rate achieved                                    | int   | Num      | na          | na         | na         |\n",
    "| exng     | exercise induced angina (1 = yes; 0 = no)                      | int   | Cat      | [1 0]       | 0          | 1          |\n",
    "| oldpeak  | ST depression induced by exercise relative to rest             | float | Num      | na          | na         | na         |\n",
    "| slp      | the slope of the peak exercise ST segment                      | int   | Cat      | [0 1 2]     | 0          | 2          |\n",
    "| caa      | number of major vessels (0-3) colored by flourosopy            | int   | Cat      | [0 1 2 3 4] | 0          | 4          |\n",
    "| thall    | 3 = normal; 6 = fixed defect; 7 = reversable defect            | int   | Cat      | [0 1 2 3]   | 0          | 3          |\n",
    "| output   | 1 (heart disease) or 0                                         | int   | Cat      | [1 0]       | 0          | 1          |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-16T05:56:31.071528Z",
     "start_time": "2021-10-16T05:56:31.025652Z"
    }
   },
   "outputs": [],
   "source": [
    "features_metadata = pd.read_csv(os.path.join('.','assets','heart_disease_features_descript.csv'))\n",
    "\n",
    "features_metadata.set_index(['Feature'], inplace=True)\n",
    "\n",
    "features_metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-16T05:56:31.801475Z",
     "start_time": "2021-10-16T05:56:31.794495Z"
    }
   },
   "outputs": [],
   "source": [
    "int(features_metadata.at['sex','Domain_max'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's update the domains for the categorical features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-16T05:56:33.479976Z",
     "start_time": "2021-10-16T05:56:33.470977Z"
    }
   },
   "outputs": [],
   "source": [
    "for cat_feat in categorical_features:\n",
    "    \n",
    "    tfdv.set_domain(schema, cat_feat, schema_pb2.IntDomain(name=cat_feat,\n",
    "                                                           min=0,\n",
    "                                                           max=int(features_metadata.at[cat_feat,'Domain_max']),\n",
    "                                                           is_categorical=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-16T05:56:34.159205Z",
     "start_time": "2021-10-16T05:56:34.154192Z"
    }
   },
   "outputs": [],
   "source": [
    "_FEATURES_ONE_HOT_DEPTHS = {}\n",
    "\n",
    "for cat_feat in categorical_features:\n",
    "    \n",
    "    _FEATURES_ONE_HOT_DEPTHS[cat_feat] = int(features_metadata.at[cat_feat,'Domain_max']) + 1\n",
    "    \n",
    "_FEATURES_ONE_HOT_DEPTHS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-16T05:56:34.734529Z",
     "start_time": "2021-10-16T05:56:34.721591Z"
    }
   },
   "outputs": [],
   "source": [
    "tfdv.display_schema(schema)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the curated schema and overwite the old schema."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-16T05:56:36.128508Z",
     "start_time": "2021-10-16T05:56:35.740184Z"
    }
   },
   "outputs": [],
   "source": [
    "tfdv.write_schema_text(schema, os.path.join(schema_uri, 'schema.pbtxt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-16T05:56:36.252740Z",
     "start_time": "2021-10-16T05:56:36.227752Z"
    }
   },
   "outputs": [],
   "source": [
    "context.show(schema_gen.outputs['schema'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id ='eg_val'></a>\n",
    "## ExampleValidator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-16T05:56:38.162864Z",
     "start_time": "2021-10-16T05:56:37.340506Z"
    }
   },
   "outputs": [],
   "source": [
    "# Instantiate ExampleValidator with the statistics and schema from the previous steps\n",
    "example_validator = ExampleValidator(\n",
    "    statistics = statistics_gen.outputs['statistics'],\n",
    "    schema = schema_gen.outputs['schema']\n",
    ")\n",
    "    \n",
    "# Run the component\n",
    "context.run(example_validator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-16T05:56:38.193810Z",
     "start_time": "2021-10-16T05:56:38.163836Z"
    }
   },
   "outputs": [],
   "source": [
    "# Visualize the output\n",
    "context.show(example_validator.outputs['anomalies'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-16T05:56:40.503780Z",
     "start_time": "2021-10-16T05:56:40.466904Z"
    }
   },
   "outputs": [],
   "source": [
    "# Some help so it's easier to copy and paste\n",
    "\n",
    "for features_list in [categorical_features, numerical_features, feature_names]:\n",
    "    print(\"[\",end=\"\")\n",
    "    for feat_idx, each_feature in enumerate(features_list):\n",
    "        if feat_idx == len(features_list)-1:\n",
    "            print(\"'\"+str(each_feature)+\"'\",end=\"\")\n",
    "        else:\n",
    "            print(\"'\"+str(features_list[feat_idx])+\"'\",end=\",\") \n",
    "    print(\"]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-16T05:56:41.222845Z",
     "start_time": "2021-10-16T05:56:41.199865Z"
    }
   },
   "outputs": [],
   "source": [
    "%%writefile {_constant_transform_module_file}\n",
    "## This file is all about writing the constants neccessary to run\n",
    "# the preprocessing_fn in the _features_transform_module_file.\n",
    "\n",
    "# Features to bucketize\n",
    "BUCKET_FEATURE_KEYS = ['age']\n",
    "\n",
    "# Number of buckets used by tf.transform for encoding each feature.\n",
    "FEATURE_BUCKET_COUNT = {'age': 10}\n",
    "\n",
    "# Feature to scale from 0 to 1\n",
    "RANGE_FEATURE_KEYS = ['oldpeak','thalachh','chol','trtbps']\n",
    "\n",
    "# Feature to apply one_hot encoding on\n",
    "ONE_HOT_KEYS = ['sex','cp','fbs','restecg','exng','slp','caa','thall']\n",
    "\n",
    "# ONE_HOT DEPTH\n",
    "FEATURE_OH_DEPTHS = {'sex': 2,\n",
    "'cp': 4,\n",
    "'fbs': 2,\n",
    "'restecg': 3,\n",
    "'exng': 2,\n",
    "'slp': 3,\n",
    "'caa': 5,\n",
    "'thall': 4}\n",
    "\n",
    "# Feature to predict\n",
    "LABEL_KEY = 'output'\n",
    "\n",
    "def transformed_name(key):\n",
    "    return key + '_xf'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-16T05:56:42.031735Z",
     "start_time": "2021-10-16T05:56:42.019721Z"
    }
   },
   "outputs": [],
   "source": [
    "%%writefile {_features_transform_module_file}\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_transform as tft\n",
    "\n",
    "import constants_transform # The .py module file name NOT the variable to which that is assigned to.\n",
    "\n",
    "_BUCKETIZED_FEATURE_KEYS = constants_transform.BUCKET_FEATURE_KEYS # 1\n",
    "_BUCKETIZED_FEATURE_COUNT = constants_transform.FEATURE_BUCKET_COUNT\n",
    "_SCALE_0_1_FEATURE_KEYS = constants_transform.RANGE_FEATURE_KEYS # 2\n",
    "_SCALE_ONE_HOT_FEATURE_KEYS = constants_transform.ONE_HOT_KEYS # 3\n",
    "_FEATURES_ONE_HOT_DEPTHS = constants_transform.FEATURE_OH_DEPTHS\n",
    "_LABEL_KEY = constants_transform.LABEL_KEY\n",
    "_transformed_name = constants_transform.transformed_name\n",
    "\n",
    "def preprocessing_fn(inputs):\n",
    "\n",
    "    features_dict = {}\n",
    "\n",
    "    for feature in _SCALE_0_1_FEATURE_KEYS:\n",
    "        data_col = inputs[feature] \n",
    "        # Transform using scaling of 0 to 1 function\n",
    "        features_dict[_transformed_name(feature)] = tft.scale_to_0_1(\n",
    "            data_col\n",
    "        )\n",
    "\n",
    "    for feature in _SCALE_ONE_HOT_FEATURE_KEYS:\n",
    "        data_col = inputs[feature] \n",
    "        # Transform using one_hot\n",
    "        features_dict[_transformed_name(feature)] = tf.one_hot(\n",
    "            data_col,\n",
    "            depth = _FEATURES_ONE_HOT_DEPTHS[feature]\n",
    "        )\n",
    "\n",
    "    for feature in _BUCKETIZED_FEATURE_KEYS:\n",
    "        data_col = inputs[feature] \n",
    "        # Transform using vocabulary available in column\n",
    "        # Hint: Use tft.compute_and_apply_vocabulary\n",
    "        features_dict[_transformed_name(feature)] = tft.bucketize(\n",
    "            data_col,\n",
    "            _BUCKETIZED_FEATURE_COUNT[feature]\n",
    "        )\n",
    "        \n",
    "    # No change in the label\n",
    "    features_dict[_transformed_name(_LABEL_KEY)] = inputs[_LABEL_KEY]\n",
    "\n",
    "    return features_dict\n",
    "\n",
    "# def _fill_in_missing(x):\n",
    "#     \"\"\"Replace missing values in a SparseTensor and convert to a dense tensor.\n",
    "#     Fills in missing values of `x` with '' or 0, and converts to a dense tensor.\n",
    "#     Args:\n",
    "#         x: A `SparseTensor` of rank 2.  Its dense shape should have size at most 1\n",
    "#           in the second dimension.\n",
    "#     Returns:\n",
    "#         A rank 1 tensor where missing values of `x` have been filled in.\n",
    "#     \"\"\"\n",
    "#     default_value = '' if x.dtype == tf.string else 0\n",
    "\n",
    "#     return tf.squeeze(\n",
    "#       tf.sparse.to_dense(\n",
    "#           tf.SparseTensor(x.indices, x.values, [x.dense_shape[0], 1]),\n",
    "#           default_value),\n",
    "#       axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-16T05:57:08.374445Z",
     "start_time": "2021-10-16T05:56:42.861247Z"
    }
   },
   "outputs": [],
   "source": [
    "# Instantiate the Transform component\n",
    "transform = Transform(\n",
    "    examples=example_gen.outputs['examples'],\n",
    "    schema=schema_gen.outputs['schema'],\n",
    "    module_file=_features_transform_module_file\n",
    ")\n",
    "\n",
    "context.run(transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-16T05:57:08.390404Z",
     "start_time": "2021-10-16T05:57:08.376439Z"
    }
   },
   "outputs": [],
   "source": [
    "context.show(transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-16T05:57:16.831007Z",
     "start_time": "2021-10-16T05:57:16.820037Z"
    }
   },
   "outputs": [],
   "source": [
    "transform.outputs['transformed_examples'].get()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-16T05:57:21.084649Z",
     "start_time": "2021-10-16T05:57:21.068693Z"
    }
   },
   "outputs": [],
   "source": [
    "print(train_uri)\n",
    "os.listdir(train_uri)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-16T05:57:23.047228Z",
     "start_time": "2021-10-16T05:57:23.025279Z"
    }
   },
   "outputs": [],
   "source": [
    "transform_uri = transform.outputs['transformed_examples'].get()[0].uri\n",
    "\n",
    "# Get the URI of the output artifact representing the transformed examples\n",
    "train_uri = os.path.join(transform_uri, 'Split-train')\n",
    "\n",
    "# Get the list of files in this directory (all compressed TFRecord files)\n",
    "tfrecord_filenames = [os.path.join(train_uri, name)\n",
    "                      for name in os.listdir(train_uri)]\n",
    "\n",
    "# Create a `TFRecordDataset` to read these files\n",
    "transformed_dataset = tf.data.TFRecordDataset(tfrecord_filenames, compression_type=\"GZIP\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-16T05:57:23.893768Z",
     "start_time": "2021-10-16T05:57:23.849884Z"
    }
   },
   "outputs": [],
   "source": [
    "# Get 3 records from the dataset\n",
    "sample_records_xf = get_records(transformed_dataset, 3)\n",
    "\n",
    "# Print the output\n",
    "pp.pprint(sample_records_xf)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "```python\n",
    "# Modules for Trainer component\n",
    "\n",
    "_input_fn_module_file = 'inputfn_trainer.py'\n",
    "_constants_train_module_file = 'constants_trainer.py'\n",
    "_model_trainer_module_file = 'model_trainer.py'\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-16T05:57:33.927410Z",
     "start_time": "2021-10-16T05:57:33.902438Z"
    }
   },
   "outputs": [],
   "source": [
    "%%writefile {_constants_train_module_file}\n",
    "## This file is all about writing the constants neccessary to run\n",
    "# the preprocessing_fn in the _features_transform_module_file.\n",
    "\n",
    "# Features to bucketize\n",
    "BUCKET_FEATURE_KEYS = ['age']\n",
    "\n",
    "# Number of buckets used by tf.transform for encoding each feature.\n",
    "FEATURE_BUCKET_COUNT = {'age': 10}\n",
    "\n",
    "# Feature to scale from 0 to 1\n",
    "RANGE_FEATURE_KEYS = ['oldpeak','thalachh','chol','trtbps']\n",
    "\n",
    "# Feature to apply one_hot encoding on\n",
    "ONE_HOT_KEYS = ['sex','cp','fbs','restecg','exng','slp','caa','thall']\n",
    "\n",
    "# LIST CONTAINING ALL FEATURE NAMES\n",
    "ALL_FEATURE_KEYS = ['age',\n",
    "'sex',\n",
    "'cp',\n",
    "'trtbps',\n",
    "'chol',\n",
    "'fbs',\n",
    "'restecg',\n",
    "'thalachh',\n",
    "'exng',\n",
    "'oldpeak',\n",
    "'slp',\n",
    "'caa',\n",
    "'thall']\n",
    "#'output']\n",
    "\n",
    "# ONE_HOT DEPTH\n",
    "FEATURES_OH_DEPTHS = {'sex': 2,\n",
    "'cp': 4,\n",
    "'fbs': 2,\n",
    "'restecg': 3,\n",
    "'exng': 2,\n",
    "'slp': 3,\n",
    "'caa': 5,\n",
    "'thall': 4}\n",
    "\n",
    "# FEATURES INPUT SHAPES\n",
    "FEATURES_INPUT_SHAPES = {'age': {'shape': 1},\n",
    "'sex': {'shape': 2},\n",
    "'cp': {'shape': 4},\n",
    "'trtbps': {'shape': 1},\n",
    "'chol': {'shape': 1},\n",
    "'fbs': {'shape': 2},\n",
    "'restecg': {'shape': 3},\n",
    "'thalachh': {'shape': 1},\n",
    "'exng': {'shape': 2},\n",
    "'oldpeak': {'shape': 1},\n",
    "'slp': {'shape': 3},\n",
    "'caa': {'shape': 5},\n",
    "'thall': {'shape': 4}}\n",
    "\n",
    "# Feature to predict\n",
    "LABEL_KEY = 'output'\n",
    "\n",
    "# Model Specific Parameters\n",
    "BATCH_SIZE = 32\n",
    "HIDDEN_UNITS = [32, 32, 32]\n",
    "LEARNING_RATE = 0.001\n",
    "NUM_EPOCHS = 1000\n",
    "\n",
    "def transformed_name(key):\n",
    "    return key + '_xf'\n",
    "\n",
    "def nll(y_true, y_pred):\n",
    "    return -y_pred.log_prob(y_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-16T05:57:34.749043Z",
     "start_time": "2021-10-16T05:57:34.731090Z"
    }
   },
   "outputs": [],
   "source": [
    "%%writefile {_model_trainer_module_file}\n",
    "###############################################\n",
    "# REFERENCE\n",
    "# https://www.tensorflow.org/tfx/tutorials/tfx/components_keras\n",
    "###############################################\n",
    "\n",
    "from typing import List, Text\n",
    "\n",
    "import os\n",
    "import absl\n",
    "import datetime\n",
    "import tensorflow as tf\n",
    "import tensorflow_transform as tft\n",
    "import tfx\n",
    "\n",
    "from tfx_bsl.public import tfxio\n",
    "\n",
    "# Imports related to the model\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import tensorflow.keras.backend as K\n",
    "import tensorflow_probability as tfp\n",
    "\n",
    "tfd = tfp.distributions\n",
    "tfpl = tfp.layers\n",
    "\n",
    "import constants_trainer\n",
    "\n",
    "_BUCKETIZED_FEATURE_KEYS = constants_trainer.BUCKET_FEATURE_KEYS # 1\n",
    "_BUCKETIZED_FEATURE_COUNT = constants_trainer.FEATURE_BUCKET_COUNT\n",
    "_SCALE_0_1_FEATURE_KEYS = constants_trainer.RANGE_FEATURE_KEYS # 2\n",
    "_SCALE_ONE_HOT_FEATURE_KEYS = constants_trainer.ONE_HOT_KEYS # 3\n",
    "_FEATURES_ONE_HOT_DEPTHS = constants_trainer.FEATURES_OH_DEPTHS\n",
    "_FEATURES_INPUT_SHAPES = constants_trainer.FEATURES_INPUT_SHAPES\n",
    "_LABEL_KEY = constants_trainer.LABEL_KEY\n",
    "_transformed_name = constants_trainer.transformed_name\n",
    "_ALL_FEATURE_KEYS = constants_trainer.ALL_FEATURE_KEYS\n",
    "\n",
    "# Model Specific Parameters\n",
    "_BATCH_SIZE = constants_trainer.BATCH_SIZE\n",
    "_HIDDEN_UNITS = constants_trainer.HIDDEN_UNITS\n",
    "_LEARNING_RATE = constants_trainer.LEARNING_RATE\n",
    "_NUM_EPOCHS = constants_trainer.NUM_EPOCHS\n",
    "_nll = constants_trainer.nll\n",
    "\n",
    "def _transformed_names(keys):\n",
    "    return [_transformed_name(key) for key in keys]\n",
    "\n",
    "def _get_serve_tf_examples_fn(model, tf_transform_output):\n",
    "    \"\"\"Returns a function that parses a serialized tf.Example and applies TFT.\"\"\"\n",
    "\n",
    "    model.tft_layer = tf_transform_output.transform_features_layer()\n",
    "    \n",
    "    @tf.function\n",
    "    def serve_tf_examples_fn(serialized_tf_examples):\n",
    "        \"\"\"Returns the output to be used in the serving signature.\"\"\"\n",
    "        feature_spec = tf_transform_output.raw_feature_spec()\n",
    "        feature_spec.pop(_LABEL_KEY)\n",
    "        parsed_features = tf.io.parse_example(serialized_tf_examples, feature_spec)\n",
    "        transformed_features = model.tft_layer(parsed_features)\n",
    "        return model(transformed_features)\n",
    "\n",
    "    return serve_tf_examples_fn\n",
    "\n",
    "def _input_fn(file_pattern: List[Text],\n",
    "              data_accessor: tfx.components.trainer.fn_args_utils.DataAccessor,\n",
    "              # Updated tfx.components.DataAccessor to tfx.components.trainer.fn_args_utils.DataAccessor\n",
    "              tf_transform_output: tft.TFTransformOutput,\n",
    "              batch_size: _BATCH_SIZE) -> tf.data.Dataset:\n",
    "    \n",
    "    \"\"\"Generates features and label for tuning/training.\n",
    "\n",
    "    Args:\n",
    "    file_pattern: List of paths or patterns of input tfrecord files.\n",
    "    data_accessor: DataAccessor for converting input to RecordBatch.\n",
    "    tf_transform_output: A TFTransformOutput.\n",
    "    batch_size: representing the number of consecutive elements of returned\n",
    "    dataset to combine in a single batch\n",
    "\n",
    "    Returns:\n",
    "    A dataset that contains (features, indices) tuple where features is a\n",
    "    dictionary of Tensors, and indices is a single Tensor of label indices.\n",
    "    \"\"\"\n",
    "    return data_accessor.tf_dataset_factory(\n",
    "      file_pattern,\n",
    "      tfxio.TensorFlowDatasetOptions(\n",
    "          batch_size=batch_size, label_key=_transformed_name(_LABEL_KEY),\n",
    "          drop_final_batch=False),\n",
    "      tf_transform_output.transformed_metadata.schema)\n",
    "\n",
    "def _create_model_inputs(feature_names = _ALL_FEATURE_KEYS): \n",
    "    inputs = {}\n",
    "    for feature_name in feature_names:\n",
    "        # Model takes 'age_fx' as names for inputs instead of 'age', therefore\n",
    "        # _transformed_name() is there to append 'fx' after the feature name.\n",
    "        inputs[_transformed_name(feature_name)] = layers.Input(\n",
    "            name=_transformed_name(feature_name), shape=(1,_FEATURES_INPUT_SHAPES[feature_name]['shape']),\n",
    "            dtype=tf.float32\n",
    "        )\n",
    "    return inputs\n",
    "\n",
    "def _create_bnn_model(train_size = _BATCH_SIZE, hidden_units = _HIDDEN_UNITS,\n",
    "                     learning_rate = _LEARNING_RATE, loss = _nll):\n",
    "    \n",
    "    divergence_fn = lambda q, p, _:tfd.kl_divergence(q, p) / train_size\n",
    "    \n",
    "    inputs = _create_model_inputs()\n",
    "    features_ = keras.layers.concatenate(list(inputs.values()))\n",
    "    features_ = keras.layers.BatchNormalization()(features_)\n",
    "\n",
    "    # Create hidden layers with weight uncertainty using the DenseVariational layer.\n",
    "    for units in hidden_units:\n",
    "        features_ = tfpl.DenseReparameterization(\n",
    "            units = units, activation = 'relu',\n",
    "            kernel_prior_fn = tfpl.default_multivariate_normal_fn,\n",
    "            kernel_posterior_fn = tfpl.default_mean_field_normal_fn(is_singular = False),\n",
    "            kernel_divergence_fn = divergence_fn,\n",
    "            bias_prior_fn = tfpl.default_multivariate_normal_fn,\n",
    "            bias_posterior_fn = tfpl.default_mean_field_normal_fn(is_singular = False),\n",
    "            bias_divergence_fn = divergence_fn\n",
    "        )(features_)\n",
    "    \n",
    "    distribution_params = tfpl.DenseReparameterization(\n",
    "            units = tfp.layers.OneHotCategorical.params_size(2), activation = None,\n",
    "            kernel_prior_fn = tfpl.default_multivariate_normal_fn,\n",
    "            kernel_posterior_fn = tfpl.default_mean_field_normal_fn(is_singular = False),\n",
    "            kernel_divergence_fn = divergence_fn,\n",
    "            bias_prior_fn = tfpl.default_multivariate_normal_fn,\n",
    "            bias_posterior_fn = tfpl.default_mean_field_normal_fn(is_singular = False),\n",
    "            bias_divergence_fn = divergence_fn\n",
    "        )(features_)\n",
    "\n",
    "    outputs = tfp.layers.OneHotCategorical(2,\n",
    "                                          convert_to_tensor_fn=tfd.Distribution.mode)(distribution_params)\n",
    "\n",
    "    model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "    \n",
    "    model.compile(\n",
    "        optimizer=tf.keras.optimizers.RMSprop(learning_rate=learning_rate),\n",
    "        loss=loss,\n",
    "        metrics = ['accuracy'],\n",
    "        experimental_run_tf_function=False\n",
    "    )\n",
    "    \n",
    "    return model\n",
    "\n",
    "# TFX Trainer will call this function.\n",
    "def run_fn(fn_args: tfx.components.trainer.fn_args_utils.FnArgs):\n",
    "           # Updated tfx.components.FnArgs to tfx.components.trainer.fn_args_utils.FnArgs\n",
    "    \"\"\"Train the model based on given args.\n",
    "\n",
    "    Args:\n",
    "    fn_args: Holds args used to train the model as name/value pairs.\n",
    "    \"\"\"\n",
    "\n",
    "    tf_transform_output = tft.TFTransformOutput(fn_args.transform_output)\n",
    "\n",
    "    train_dataset = _input_fn(fn_args.train_files, fn_args.data_accessor, \n",
    "                            tf_transform_output, _BATCH_SIZE)\n",
    "    eval_dataset = _input_fn(fn_args.eval_files, fn_args.data_accessor, \n",
    "                           tf_transform_output, _BATCH_SIZE)\n",
    "\n",
    "\n",
    "    model = _create_bnn_model()\n",
    "    \n",
    "    model.fit(\n",
    "    train_dataset,\n",
    "    steps_per_epoch=fn_args.train_steps,\n",
    "    validation_data=eval_dataset,\n",
    "    validation_steps=fn_args.eval_steps,\n",
    "    epochs = 10) #,\n",
    "    \n",
    "    signatures = {\n",
    "      'serving_default':\n",
    "          _get_serve_tf_examples_fn(model,\n",
    "                                    tf_transform_output).get_concrete_function(\n",
    "              tf.TensorSpec(shape=[None],\n",
    "                            dtype=tf.string,\n",
    "                            name='examples')\n",
    "          )\n",
    "    }\n",
    "    \n",
    "    tf.keras.models.save_model(model, fn_args.serving_model_dir, save_format='tf', signatures=signatures)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-16T05:58:18.551738Z",
     "start_time": "2021-10-16T05:57:52.656669Z"
    }
   },
   "outputs": [],
   "source": [
    "train_steps = int(np.ceil(212//32))\n",
    "eval_steps = int(np.ceil(91//32))\n",
    "\n",
    "trainer = tfx.components.Trainer(\n",
    "    module_file=os.path.abspath(_model_trainer_module_file),\n",
    "    examples=transform.outputs['transformed_examples'],\n",
    "    transform_graph=transform.outputs['transform_graph'],\n",
    "    schema=schema_gen.outputs['schema'],\n",
    "    train_args=trainer_pb2.TrainArgs(splits=['train'], num_steps=train_steps),\n",
    "    eval_args=trainer_pb2.EvalArgs(splits=['eval'], num_steps=eval_steps)\n",
    ")\n",
    "\n",
    "context.run(trainer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-16T05:58:18.566738Z",
     "start_time": "2021-10-16T05:58:18.553733Z"
    }
   },
   "outputs": [],
   "source": [
    "trainer.outputs['model'].get()[0].uri"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name = 'pusher'></a>\n",
    "\n",
    "# Pusher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-16T05:58:23.874802Z",
     "start_time": "2021-10-16T05:58:22.589941Z"
    }
   },
   "outputs": [],
   "source": [
    "pusher = Pusher(\n",
    "    model=trainer.outputs['model'],\n",
    "    push_destination=pusher_pb2.PushDestination(\n",
    "        filesystem=pusher_pb2.PushDestination.Filesystem(\n",
    "            base_directory=_serving_model_dir)))\n",
    "context.run(pusher)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-16T05:58:30.929327Z",
     "start_time": "2021-10-16T05:58:25.974584Z"
    }
   },
   "outputs": [],
   "source": [
    "push_uri = pusher.outputs.pushed_model.get()[0].uri\n",
    "model = tf.saved_model.load(push_uri)\n",
    "\n",
    "for item in model.signatures.items():\n",
    "    pp.pprint(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-16T09:32:48.768563Z",
     "start_time": "2021-10-16T09:32:48.659753Z"
    }
   },
   "outputs": [],
   "source": [
    "# Get prediction function from serving\n",
    "f = model.signatures['serving_default']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Complete Pipeline\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running it locally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-16T05:58:37.052986Z",
     "start_time": "2021-10-16T05:58:36.929007Z"
    }
   },
   "outputs": [],
   "source": [
    "from tfx.orchestration.local.local_dag_runner import LocalDagRunner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-16T05:58:39.968875Z",
     "start_time": "2021-10-16T05:58:39.944893Z"
    }
   },
   "outputs": [],
   "source": [
    "# https://github.com/tensorflow/tfx/blob/master/tfx/examples/chicago_taxi_pipeline/\n",
    "\n",
    "def _create_pipeline(pipeline_name: str, pipeline_root: str, data_root: str,\n",
    "                     schema_path: str, transform_module_file: str, trainer_module_file: str,\n",
    "                     serving_model_dir: str, metadata_path: str):\n",
    "    \"\"\"Implements the heart disease pipeline with TFX.\"\"\"\n",
    "    \n",
    "    # Brings data into the pipeline or otherwise joins/converts training data.\n",
    "    example_gen = tfx.components.CsvExampleGen(\n",
    "        input_base=data_root\n",
    "    )\n",
    "\n",
    "    # Computes statistics over data for visualization and example validation.\n",
    "    statistics_gen = tfx.components.StatisticsGen(\n",
    "        examples=example_gen.outputs['examples']\n",
    "    )\n",
    "\n",
    "  # Import the schema.\n",
    "    schema_importer = tfx.dsl.components.common.importer.Importer(\n",
    "        source_uri=schema_path,\n",
    "        artifact_type=tfx.types.standard_artifacts.Schema\n",
    "    ).with_id('schema_importer')\n",
    "\n",
    "  # Performs anomaly detection based on statistics and data schema.\n",
    "    example_validator = tfx.components.ExampleValidator(\n",
    "        statistics=statistics_gen.outputs['statistics'],\n",
    "        schema=schema_importer.outputs['result']\n",
    "    )\n",
    "\n",
    "  # NEW: Transforms input data using preprocessing_fn in the 'module_file'.\n",
    "    transform = tfx.components.Transform(\n",
    "        examples=example_gen.outputs['examples'],\n",
    "        schema=schema_importer.outputs['result'],\n",
    "        materialize=False,\n",
    "        module_file=transform_module_file\n",
    "    )\n",
    "    \n",
    "    train_steps = int(np.ceil(212//32))\n",
    "    eval_steps = int(np.ceil(91//32))\n",
    "\n",
    "    # Uses user-provided Python function that trains a model.\n",
    "    trainer = tfx.components.Trainer(\n",
    "        module_file=trainer_module_file,\n",
    "        examples=example_gen.outputs['examples'],\n",
    "\n",
    "    # NEW: Pass transform_graph to the trainer.\n",
    "        transform_graph=transform.outputs['transform_graph'],\n",
    "\n",
    "        train_args=trainer_pb2.TrainArgs(splits=['train'], num_steps=train_steps),\n",
    "        eval_args=trainer_pb2.EvalArgs(splits=['eval'], num_steps=eval_steps)\n",
    "    )\n",
    "\n",
    "    # Pushes the model to a filesystem destination.\n",
    "    pusher = tfx.components.Pusher(\n",
    "        model=trainer.outputs['model'],\n",
    "        push_destination=pusher_pb2.PushDestination(\n",
    "          filesystem=pusher_pb2.PushDestination.Filesystem(\n",
    "              base_directory=serving_model_dir)\n",
    "        )\n",
    "    )\n",
    "\n",
    "    components = [\n",
    "        example_gen,\n",
    "        statistics_gen,\n",
    "        schema_importer,\n",
    "        example_validator,\n",
    "        transform,  # NEW: Transform component was added to the pipeline.\n",
    "        trainer,\n",
    "        pusher\n",
    "    ]\n",
    "    \n",
    "    # https://www.tensorflow.org/tfx/guide/build_tfx_pipeline\n",
    "    # https://github.com/tensorflow/tfx/blob/master/tfx/orchestration/pipeline.py\n",
    "    \n",
    "    return tfx.orchestration.pipeline.Pipeline( # Changed tfx.dsl.Pipeline to tfx.orchestration.pipeline.Pipeline\n",
    "        pipeline_name=pipeline_name,\n",
    "        pipeline_root=pipeline_root,\n",
    "        metadata_connection_config=tfx.orchestration.metadata.sqlite_metadata_connection_config(metadata_path),\n",
    "        components=components\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-16T05:58:40.925096Z",
     "start_time": "2021-10-16T05:58:40.917117Z"
    }
   },
   "outputs": [],
   "source": [
    "_PIPELINE_NAME = 'local_HD_BNN_pipeline'\n",
    "_PIPELINE_ROOT = os.path.join(os.getcwd(),'local_HD_BNN_pipeline')\n",
    "_DATA_ROOT = os.path.join(_PIPELINE_ROOT, 'data')\n",
    "_SCHEMA_PATH = os.path.join(_PIPELINE_ROOT, 'schema')\n",
    "_SERVING_MODEL_DIR = os.path.join(_PIPELINE_ROOT, 'serving_model')\n",
    "_METADATA_PATH = os.path.join(_PIPELINE_ROOT, 'metadata.sqlite')\n",
    "\n",
    "_transform_module_file = 'features_transform.py'\n",
    "_trainer_module_file = 'model_trainer.py'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-16T05:58:45.408869Z",
     "start_time": "2021-10-16T05:58:41.839150Z"
    }
   },
   "outputs": [],
   "source": [
    "LocalDagRunner().run(\n",
    "    _create_pipeline(\n",
    "        pipeline_name=_PIPELINE_NAME,\n",
    "        pipeline_root=_PIPELINE_ROOT,\n",
    "        data_root=_DATA_ROOT,\n",
    "        schema_path=_SCHEMA_PATH,\n",
    "        transform_module_file=_transform_module_file,\n",
    "        trainer_module_file=_trainer_module_file,\n",
    "        serving_model_dir=_SERVING_MODEL_DIR,\n",
    "        metadata_path=_METADATA_PATH\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An error is raised in the running of the above cell, I had raised an issue with regards to this error to TFX: [TFX-3999](https://github.com/tensorflow/tfx/issues/3999)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "This notebook took reference from, copied and adapted, some of the codes from the following:\n",
    "\n",
    "1. Machine Learning Data Lifecycle in Production by DeepLearning.AI, assignments from Week 2 and Week 3 ([Link](https://www.coursera.org/learn/machine-learning-data-lifecycle-in-production/home/welcome))\n",
    "2. Collab Notebook ([Link](https://colab.research.google.com/gist/rafiqhasan/2164304ede002f4a8bfe56e5434e1a34/dl-e2e-taxi-dataset-tfx-e2e.ipynb)) | [Youtube Tutorial](https://www.youtube.com/watch?v=VrBoQCchJQU) by Hasan\n",
    "3. Tensorflow Tutorial on TFX Pipeline for Chicago Taxi Dataset ([Link](https://www.tensorflow.org/tfx/tutorials/tfx/components_keras))\n",
    "4. Tensorflow Tutorial on TFX Pipeline for Penguin ([Link](https://tensorflow.google.cn/tfx/tutorials/tfx/penguin_tft))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
